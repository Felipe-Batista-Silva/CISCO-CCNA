# Transmission Control Protocol (TCP): O material avançado





A camada de transporte é aquela que conecta os aplicativos à rede, e o Transmission Control Protocol (TCP) é o protocolo que pode fazer isso de maneira confiável. Já explicamos as diferenças entre UDP e TCP no [artigo anterior](https://www-ictshore-com.translate.goog/free-ccna-course/transport-layer-tcp-and-udp/?_x_tr_sl=en&_x_tr_tl=pt&_x_tr_hl=nl&_x_tr_pto=nui) , e agora é hora de entender como podemos melhorar o desempenho do TCP para satisfazer os requisitos dos aplicativos modernos e usar a rede subjacente da melhor maneira possível. Ao fazer isso, aprenderemos sobre os princípios em um host que regulam a conexão TCP, como estados e operações em janelas: bem-vindo ao artigo sobre TCP avançado!

## Estados TCP

### A teoria dos estados TCP

Para executar o TCP, qualquer dispositivo deve estabelecer alguns princípios: ele deve saber que deve retransmitir quando um ACK não é recebido, deve saber que deve enviar confirmações pelos dados que recebe e assim por diante. Todas essas regras podem ser definidas em um **diagrama de estado** ou fluxograma. A qualquer momento, o dispositivo deve estar em um dos estados do diagrama, mas em apenas um deles. Com base no estado em que o dispositivo está, ele pode fazer algumas operações em vez de outras e *pode fazer a transição para outros estados específicos* . À primeira vista, o diagrama pode parecer muito complexo, mas não é. Vamos dar uma olhada e explicar o que significa.

![Diagrama de estados do TCP com a transição entre eles](https://www.ictshore.com/wp-content/uploads/2016/12/1017-01-TCP_States_diagram.png)Todos os estados possíveis de uma conexão TCP, com os itens necessários para fazer a transição de um estado para o outro.

Vamos analisar esse tópico complexo. Como você pode ver, existem **11 estados diferentes** na imagem e não *há um verdadeiro ponto de partida* . Isso significa que qualquer outro estado pode ser alcançado a partir de pelo menos outro estado (há uma seta apontando em sua direção). Cada estado é representado por um retângulo, enquanto as setas apontando para fora dele indicam os estados para os quais um dispositivo pode se mover a partir do estado atual. O texto na seta indica o que deve acontecer para que a transição de estado seja bem-sucedida. Por fim, todos os estados são agrupados em áreas de cores diferentes: isso serve apenas para agrupar estados que, juntos, têm o mesmo objetivo.



*Nota: você pode encontrar estados escritos de maneiras diferentes em livros diferentes, usando sublinhados, travessões ou até mesmo nada para unir palavras.*

Mesmo que não haja um verdadeiro ponto de partida, podemos pensar que é o `CLOSED`estado: neste estado, a conexão não pode nem mesmo ser aberta. Isso ocorre porque qualquer conexão começa sem conexão; pense nisso, originalmente não há conexão, mas depois com o handshake de três vias, ela é negociada. Um dispositivo disposto a abrir uma conexão pode passar de um `CLOSED`estado para dois outros estados, dependendo de sua função. Vamos pensar primeiro no lado do servidor (conhecido como **Responder** aqui). O servidor não inicia conexões para o cliente, mas, em vez disso, espera que os clientes façam solicitações. Isso significa que o servidor deve estar pronto para aceitar solicitações e, para isso, muda de `CLOSED`para`LISTEN`estado assim que o aplicativo for iniciado. Este estado indica que o servidor está pronto para receber conexão e irá aguardar por SYN de qualquer cliente.

O dispositivo que decide iniciar ativamente uma conexão é conhecido como **Iniciador** no diagrama e *geralmente é o cliente* . Ele se move de um `CLOSED`estado para outro `SYN_SENT`apenas enviando o SYN para o servidor. Assim que o respondente recebe o SYN enquanto está no `LISTEN`estado, ele responde com SYN + ACK e passa imediatamente para o `SYN_RECEIVED`estado. Depois que o iniciador recebe essa mensagem, ele passa para o `ESTABLISHED`estado enviando um ACK para o respondente, que fará a transição para o `ESTABLISHED`estado assim que receber o ACK. Isso significa que a conexão foi estabelecida e os dados podem ser trocados.

*Nota: quando uma conexão é iniciada, o Bloco de Controle de Transmissão (TCB) é definido. É um conjunto de informações e variáveis a serem armazenadas na memória do dispositivo durante todo o tempo de conexão. Isso inclui as portas de origem e destino.*

Assim que a troca de dados for concluída, alguns dos dois dispositivos desejarão fechar a conexão. Mesmo neste caso, temos um dispositivo iniciando o fechamento (Iniciador) e um respondente para ele, mas isso não significa necessariamente que um seja o cliente e o outro o servidor. O iniciador é apenas o dispositivo que inicia ativamente o processo de fechamento, enquanto o respondente é aquele que responde ao iniciador (fechamento passivo). O iniciador enviará um FIN, passando de `ESTABLISHED`para `FIN_WAIT_1`estado, e a resposta irá para `CLOSE_WAIT`estado, assim que recebe essa FIN, enviando um ACK em resposta. Quando o iniciador recebe esse ACK, ele apenas passa para o `FIN_WAIT_2`estado e fica sentado sem fazer nada. O respondente ainda está no`CLOSE_WAIT`estado, pois está esperando que o aplicativo termine todas as suas coisas e feche. Assim que o aplicativo terminar, o dispositivo enviará um FIN ao iniciador para informar que ele também deseja fechar a conexão e, ao fazer isso, ele passa para o `LAST_ACK`estado. Assim que o iniciador receber o FIN do respondente, ele enviará um ACK e passará para o `TIME_WAIT`estado. Assim que o respondente receber o ACK, ele voltará ao `CLOSED`estado. Após um temporizador de dois milissegundos, o iniciador também mudará para o `CLOSED`estado. A conexão agora foi encerrada de acordo com ambas as partes envolvidas.

### Estados do protocolo de controle de transmissão aplicados

Pode parecer um pouco complexo, mas assim que você o vir **aplicado a uma conexão** , começará a fazer sentido. Aqui está uma conexão TCP típica com os estados pelos quais cada dispositivo passa.

![Estados do TCP em uma conexão](https://www.ictshore.com/wp-content/uploads/2016/12/1017-02-TCP_States_in_a_connection.png)Um exemplo de conexão TCP com os estados em que o cliente e o servidor são executados durante a evolução da conexão.

Antes de começarmos com a explicação, lembre-se de que os *estados são significativos para a conexão com o dispositivo* : eles não são estados da conexão em si, portanto, um dispositivo pode estar em um estado e o outro em um estado diferente. Como você pode ver, o cliente começa com o `CLOSED`estado, mas o servidor já está no `LISTEN`estado porque o aplicativo no servidor que precisa ouvir as solicitações e criar respostas já foi iniciado. O cliente envia um SYN e passa para o `SYN_SENT`estado. Assim que o servidor receber esse SYN, ele passará para o `SYN_RECEIVED`estado enviando SYN + ACK de volta ao cliente, que passará para o `ESTABLISHED`estado e enviará um ACK em resposta. Assim que esse ACK for recebido, o servidor também mudará para o `ESTABLISHED`estado.

*Nota: o servidor faz a transição do estado LISTEN para o estado SYN_RECEIVED para cada conexão que recebe, pois pode aceitar várias conexões de clientes diferentes ao mesmo tempo. Isso significa que toda vez que ele receber um SYN, ele criará uma instância dedicada do diagrama de estado para essa conexão, que será destruída quando a conexão for finalizada. Dessa forma, sempre haverá uma instância no estado LISTEN pronta para aceitar novas conexões de novos clientes.*

Assim que a troca de dados for concluída, é o cliente neste exemplo que deseja encerrar a conexão. Ele faz isso enviando um FIN e fazendo a transição para o `FIN_WAIT_1`estado. Quando o servidor recebe esse FIN, ele responderá com um ACK e mudará para o `CLOSE_WAIT`estado. Quando este ACK for recebido pelo cliente, ele passará para o `FIN_WAIT_2`estado, aguardando o fechamento do servidor. Assim que o servidor terminar o que estava fazendo, ele também envia um FIN, passando para o `LAST_ACK`estado. Quando esse FIN é recebido, o cliente envia um ACK e passa para o estado TIME_WAIT e, após dois milissegundos, para o `CLOSED`estado. Quando esse ACK é recebido, o servidor também muda para o `CLOSED`estado. A conexão foi encerrada.

### Fechamento Simultâneo

Há um caso interessante no qual vale a pena dedicar algum tempo: o **fechamento simultâneo** de uma conexão TCP. Nesse caso, não há iniciador ativo e respondedor passivo para o fechamento, mas em vez disso, *ambos os dispositivos estão tentando fechar* a conexão *ativamente ao* mesmo tempo.

![Com o fechamento simultâneo do TCP, o fechamento é iniciado por ambos os parceiros](https://www.ictshore.com/wp-content/uploads/2016/12/1017-03-Simultaneous_close.png)Em um fechamento simultâneo de TCP, os dois parceiros TCP iniciam o processo de fechamento ativo ao mesmo tempo.

Como você pode ver na foto, esse processo de fechamento é perfeitamente **simétrico** . O cliente envia um FIN ao servidor para fechar a conexão e passa para o `FIN_WAIT_1`estado. Ao mesmo tempo, o servidor envia um FIN para o cliente com a mesma intenção, passando para o `FIN_WAIT_1`estado também. O cliente recebe um FIN, então passa para o `CLOSING`estado de envio de um ACK. O mesmo vai para o servidor, que também recebe um FIN e passa para o `CLOSING`estado enviando um ACK. Quando qualquer um dos dois dispositivos recebe o ACK, ele passa para o `TIME_WAIT`estado e, após o tempo limite, para o `CLOSED`estado. Basicamente, com um fechamento simultâneo, um flag FIN é recebido enquanto ainda está no `FIN_WAIT_1`estado.

Também há uma **abertura de conexão simultânea** que funciona da mesma maneira (um SYN é recebido enquanto no `SYN_SENT`estado), mas é muito raro encontrar na vida real, pois o modelo cliente-servidor é o dominante, e neste tipo de arquitetura é sempre um dispositivo que decide se conectar ao outro, não a ambos.

### Reinicialização de uma conexão TCP

Todos nós sabemos que um dispositivo pode usar o sinalizador RST para mover para o `CLOSED`estado a qualquer momento que decida fazê-lo. Isso é destacado na figura a seguir, que mostra os dois casos possíveis que podem ocorrer quando um RST é emitido.

![Redefinição da conexão TCP, também no caso de perda do sinalizador RST](https://www.ictshore.com/wp-content/uploads/2016/12/1017-04-Reset_connection.png)Uma reconfiguração de conexão em TCP é unidirecional e encerra imediatamente a conexão do emissor. Se o parceiro receber o segmento, ele encerrará imediatamente a conexão também, caso contrário, o tempo limite será atingido.

Na primeira foto, o reset é **enviado e recebido** . Como você pode ver, ele pode realmente ser enviado enquanto está em qualquer outro estado e, assim que o dispositivo o envia, ele fecha a conexão imediatamente. Isso significa que ele não está disposto a ouvir nem aceitar mais nada dessa conexão, portanto, quando o outro dispositivo receber o RST, não terá outra escolha a não ser encerrar a conexão imediatamente.

O outro caso, explicado na segunda foto, é o caso em que o **RST é perdido** . Como este é o último segmento enviado, nenhuma retransmissão será implementada para ele. O dispositivo que o está enviando fechará a conexão imediatamente e não processará mais pacotes para essa conexão. Desta forma, o outro dispositivo que não recebeu o RST ainda tentará enviar dados mas, como não receberá nenhum reconhecimento, considerará que a conexão foi encerrada após um timeout.

## TCP Windowing

No *cabeçalho do segmento TCP* , podemos encontrar um campo longo de 16 bits chamado **Window Size** . Ele identifica a **janela** do **receptor** , que é o buffer de entrada para aquela conexão no dispositivo que envia aquele segmento. Vamos declarar em um inglês claro. Tamanho da janela é o número de bytes que o dispositivo que está enviando o segmento pode armazenar esperando que o aplicativo os processe. Isso se aplica apenas aos dados recebidos. Ao enviar esse valor em um segmento, o dispositivo está dizendo *“Ei, você pode me enviar X bytes e eu garanto que irei armazená-los na minha memória temporária para que o aplicativo os processe eventualmente”* , onde *X* é o valor do tamanho da janela .

O tamanho da janela é enviado em *cada segmento* , mas é o outro dispositivo que deve *controlá-lo* : o tamanho da janela indica o **tamanho total da janela do receptor** e não especifica quanto já foi usado. Para controlar a parte da memória já em uso no outro dispositivo, um dispositivo deve *comparar os dados enviados com os dados confirmados* . Cada segmento enviado é assumido para ocupar espaço na janela do receptor do outro dispositivo, e cada segmento reconhecido é assumido como removido da janela do receptor, esvaziando algum espaço. Lembre-se de que todas essas operações não são realizadas considerando o número de segmentos, mas sim o número de bytes em cada segmento. Vamos dar uma olhada na imagem a seguir para esclarecer isso.

![Processo de janelamento TCP, a janela é redimensionada de acordo com a capacidade de processamento de dados](https://www.ictshore.com/wp-content/uploads/2016/12/1017-05-TCP_Windowing.png)Exemplo de processo de janelamento TCP, com a janela do receptor redimensionada pelo cliente devido ao baixo valor inicial de processamento de dados.

Para simplificar ainda mais, chamamos o cliente de “A” e o servidor de “B”. Ao negociar a conexão com o handshake triplo, os dois dispositivos informam um ao outro seu tamanho de janela: A tem uma **janela de receptor** *(RWIN ou RWND)* de 32 KB, enquanto B tem uma janela de receptor de 16 KB. Cada dispositivo se lembrará da janela do receptor do outro dispositivo e a considerará como o espaço disponível no buffer do outro dispositivo. No entanto, uma vez que nenhuma informação é trocada sobre *"quão cheio o buffer está"*, o dispositivo deve acompanhá-lo. A boa notícia é que a única coisa que preenche o buffer do outro dispositivo são os dados que enviamos para ele: sabemos a capacidade total do buffer e sabemos que ele começa vazio, mas também sabemos quantos dados estamos enviando e quantos dados são reconhecidos. Este exemplo é propositalmente simples: o cliente envia 1K de dados todas as vezes e, portanto, diminui o espaço disponível no buffer do servidor de 1K. Ele faz isso três vezes, enquanto isso, o servidor começa a processar os dados e enviar as confirmações de volta. A cada confirmação recebida, o cliente verifica quantos bytes são confirmados e aumenta o espaço disponível desse número de bytes.

Posteriormente, é o servidor que deseja enviar os dados ao cliente. No entanto, ele tem uma grande carga de dados para transmitir: envia 1k por vez, 20 vezes (muitos são omitidos na foto), reduzindo o espaço disponível de 32k para 11k. O cliente pode ter dificuldade em processar dados e, no caso de começar a ficar sobrecarregado (como neste caso), pode dizer ao outro dispositivo para **interromper a transmissão** , enviando um tamanho de janela igual a zero.

*Nota: O tamanho da janela é enviado em cada segmento e, em cada segmento, define o espaço total disponível no buffer do dispositivo que o está enviando, a menos que esse valor seja diferente do recebido originalmente durante o handshake triplo (ou o último, se mudou várias vezes durante a conexão), o dispositivo não vai considerá-lo.*

Isso ocorre porque um dispositivo transmitirá tantos bytes quanto puder, mas *o número de bytes não confirmados não pode exceder o tamanho da janela* . O tamanho da janela do receptor foi projetado especificamente para evitar dispositivos sobrecarregados, pois se o buffer estiver cheio, qualquer segmento recebido que não couber no buffer será descartado. Com dispositivos modernos, *raramente é enfatizado* . Ele pode ser usado em conjunto com a **janela de congestionamento** para lidar com o **congestionamento da** rede, como explicaremos posteriormente neste artigo.

## Reconhecimento seletivo

O reconhecimento seletivo, também conhecido como **ACK seletivo** ou simplesmente **SACK** ( [RFC 2018](https://translate.google.com/website?sl=en&tl=pt&nui=1&u=https://tools.ietf.org/html/rfc2018) ), é outra melhoria no desempenho do TCP que permite que um dispositivo reconheça segmentos individualmente. Na implementação tradicional do TCP, quando um segmento é perdido, ele deve ser retransmitido, e todos os segmentos após o perdido também devem ser retransmitidos, mesmo que tenham sido recebidos corretamente. Isso ocorre devido à natureza do número de confirmação, que informa ao outro dispositivo qual byte é esperado em seguida. Com essa lógica, podemos reconhecer um fluxo contíguo de bytes, mas não podemos colocar lacunas nele (por exemplo, *"Ok, tenho 0-8 e depois 10-20, mas não de 8 a 10"*) O SACK faz exatamente isso, permite que um dispositivo reconheça segmentos individualmente, de modo que apenas os perdidos sejam retransmitidos, com uma melhoria geral do rendimento. Na imagem a seguir, comparamos o mecanismo de reconhecimento tradicional ao SACK.

![ACK seletivo TCP vs sistema ACK tradicional](https://www.ictshore.com/wp-content/uploads/2016/12/1017-06-TCP_selective_acknowledgment_SACK_vs_normale_ACK.png)Sistema de confirmação normal à esquerda versus sistema de confirmação seletiva (SACK) à direita. O segmento com o sinal de advertência é transmitido e recebido duas vezes no mecanismo de confirmação normal.

No primeiro caso, com o *sistema ACK tradicional* , o servidor envia os bytes 1-1460 para o cliente, depois 1461-2920 em outro segmento e 2921-4380 em um terceiro segmento. No entanto, o segundo segmento contendo bytes 1461-2920 é perdido devido a um problema temporário na rede, então o cliente reconhece o primeiro segmento (enviando um número de confirmação de 1461), mas descarta o terceiro porque se o número de confirmação fosse 4381, haveria não houve maneira de dizer sobre o segmento ausente no meio. Assim, com um número de confirmação de 1461, o servidor retransmite os bytes 1461-2920 (o segmento perdido), mas também 2921-4380, que foi entregue ao cliente com sucesso. Desta forma, estamos enviando duas vezes um segmento válido, desperdiçando largura de banda e tempo.

Para resolver isso, o SACK implementa duas opções diferentes de cabeçalho TCP. A primeira é chamada **de Opção Permitida por Sack** e tem um identificador (tipo) definido como 4. Essa opção é usada durante o handshake de três vias para verificar se ambos os parceiros TCP suportam o mecanismo SACK. Caso esta primeira fase seja bem-sucedida, a **opção Sack** (tipo configurada em 5) é usada. Explicar o formato desta opção está fora do escopo, mas saiba que é um espaço no cabeçalho TCP usado para informar ao outro dispositivo quais segmentos são reconhecidos com SACK. Se você quiser se aprofundar neste recurso, verifique o [RFC](https://translate.google.com/website?sl=en&tl=pt&nui=1&u=https://tools.ietf.org/html/rfc2018). Basicamente, quando um segmento é perdido e outros segmentos depois dele são recebidos, tudo antes do segmento perdido é reconhecido da maneira tradicional, e cada segmento após o perdido é reconhecido com o mecanismo SACK, usando os campos apropriados no TCP "Opção de Sack" **expansão do cabeçalho** . Como na imagem, uma vez que o único segmento perdido é 1461-2920, o cliente confirma com os bytes ACK tradicionais até 1461, enquanto com SACK ele confirma os bytes 2920-4380.

## Compressão de Cabeçalho

A compactação de cabeçalho é um recurso TCP interessante que permite o **aprimoramento da largura de banda** em *links de baixa velocidade* , como conexões de satélite. Este recurso é extremamente simples, mas diferente de qualquer outro, porque não é implementado em hosts TCP. Todos os outros recursos que abordamos foram aprimoramentos implementados pelos dois parceiros TCP, os dispositivos que hospedam aplicativos e se comunicam entre si, embora, neste caso, eles não estejam envolvidos de forma alguma. Em vez disso, esse recurso é **implementado nos roteadores** (dispositivos de rede) no caminho.

Imagine um roteador que precisa processar pacotes IP contendo segmentos TCP, se você estiver fazendo uso pesado de uma conexão (como baixar um arquivo de um servidor), o roteador verá muitos pacotes com os mesmos IPs de origem e destino, todos eles contendo as mesmas portas de origem e destino no cabeçalho do segmento. Tudo isso deve ser enviado ao próximo roteador no caminho para que ele possa continuar a rotear os pacotes corretamente. No entanto, os cabeçalhos ocupam largura de banda que diminui o desempenho, mesmo de maneira sensata em links de baixa velocidade. Como todos esses cabeçalhos permanecem iguais para toda a conexão, *não poderíamos enviá-los apenas uma vez?* Com a compactação de cabeçalho, *podemos* .

Um roteador que implementa a compactação de cabeçalho usa os endereços IP e TCP no cabeçalho, além de outros campos que não serão alterados durante a conexão, e executa um algoritmo neles para extrapolar um *identificador exclusivo (ID de hash)* que é muito menor. Na maioria dos casos, começamos com cabeçalhos de 40 bytes e os compactamos em um **identificador de 4 bytes** . Obviamente, todos os roteadores que recebem um cabeçalho compactado devem conhecer seu valor expandido para enviar o pacote / segmento ao destino correto. No final, um roteador descompactará o cabeçalho e enviará o normal para o dispositivo de destino (ou para outro roteador que não ofereça suporte à compactação de cabeçalho).

![Compactação do cabeçalho TCP para melhorar o desempenho em links de baixa velocidade](https://www.ictshore.com/wp-content/uploads/2016/12/1017-07-TCP_header_compression.png)Com a compactação de cabeçalho, em links de baixa velocidade, podemos reduzir cabeçalhos de 40 bytes para identificadores de 4 bytes.

Como você pode ver na imagem, os dois roteadores falam pela conexão do satélite usando IDs de hash, com cada hash que identifica exclusivamente um fluxo de dados. Nesta imagem, temos apenas fluxos unidirecionais, mas, na realidade, também haveria um ID de hash para o tráfego de resposta.

## Lidando com congestionamentos de rede

### Retransmissão rápida

A retransmissão rápida é um recurso muito simples para implementar **uma retransmissão agressiva** de dados. Em uma implementação normal de TCP, devemos esperar que os ACKs decidam o que devemos retransmitir. Com a retransmissão rápida habilitada se um *ACK não for recebido dentro de um tempo limite específico* , o segmento ainda não confirmado é reenviado automaticamente, para economizar tempo. Isso pode saturar desnecessariamente as redes de alta latência e baixa largura de banda, já que as informações não são realmente perdidas, mas simplesmente demoram para chegar do outro lado.

### Controle de congestionamento TCP

Construir redes está se tornando cada vez mais barato, podemos atingir taxas que acreditávamos impossíveis até 10 anos atrás. No entanto, à medida que a velocidade da rede aumenta, os requisitos dos aplicativos modernos também aumentam. Usamos aplicativos que consomem largura de banda diariamente, como VoIP, streaming de vídeo ou jogos online massivos. Portanto, é necessário aproveitar 100% da velocidade da rede, mas se tentarmos enviar muito mais dados do que a rede pode suportar, causaremos uma queda no desempenho. Precisamos encontrar uma maneira de usar o máximo de largura de banda possível, sem inundar a rede com tráfego e sobrecarregá-la. Precisamos de algumas ferramentas para controlar o congestionamento, e o TCP tem essas ferramentas.

Os dois parceiros TCP são apenas dois hosts na extremidade da rede, eles não podem saber como a rede inteira funciona e, portanto, não podem saber a taxa de transferência efetiva que têm entre si. Em vez disso, eles devem encontrar uma maneira de determiná-lo. Para fazer isso, usamos a **janela de congestionamento (CWND)** . Esse é o número de bytes que podem ser enviados antes de termos que parar e esperar por uma confirmação. A janela de congestionamento é local para o dispositivo e nunca compartilhada na conexão, ao contrário da janela do receptor que é enviada em todos os segmentos. A qualquer momento, um dispositivo pode enviar no máximo o número de bytes especificado pelo *mínimo entre a janela do receptor e a janela de congestionamento* , como na fórmula a seguir.

bytes transmissíveis = min ( cwnd, rwnd )

Isso significa que *se a janela de congestionamento for menor do que a janela do receptor* , o dispositivo pode transmitir até o número de bytes definido na janela de congestionamento antes de aguardar as confirmações. Em vez disso, *se a janela do receptor for menor do que a janela de congestionamento* , o dispositivo pode transmitir até o número de bytes definido na janela do receptor antes de aguardar uma confirmação.

A janela de congestionamento varia dinamicamente com base no congestionamento da rede. Cada vez que um segmento não é confirmado, presume-se que seja devido ao congestionamento da rede. A maneira como a janela de congestionamento evolui ao longo do tempo é definida em um algoritmo, que depende da implementação. Apresentaremos agora um dos mais comuns. O algoritmo segue estas regras:

1. A janela de congestionamento começa com o tamanho de um segmento (cerca de 1 KB)
2. Um limite de janela de congestionamento (ssthresh) é definido
3. Se uma confirmação for recebida, e o tamanho da janela de congestionamento atual for menor do que ssthresh, a janela de congestionamento dobra
4. Se uma confirmação for recebida, mas a janela de congestionamento for maior ou igual a sshthresh, a janela de congestionamento aumentará em seu valor inicial (por exemplo, 1 KB)
5. Se um segmento não for reconhecido para que a retransmissão seja acionada, a janela de congestionamento é cortada pela metade e ssthresh é colocado neste valor
6. A janela de congestionamento não pode ser maior que a janela do receptor

Para simplificar a explicação, criamos uma troca de segmento de amostra e aplicamos o mecanismo de controle de congestionamento a ela. Como você pode ver na imagem abaixo, cada dispositivo rastreia sua própria *janela de congestionamento (CWND, verde)* e da *janela* do *receptor (RWND)* do parceiro. À esquerda, numeramos a troca de segmento (linha-coluna) para nos referirmos à única linha posteriormente.

![Controle de congestionamento TCP com exemplo de janela de congestionamento](https://www.ictshore.com/wp-content/uploads/2016/12/1017-08-TCP_congestion_control.png)O TCP implementa a janela de congestionamento para controlar quantos bytes podem ser deixados sem confirmação antes de pausar a transmissão

Ao negociar a conexão, os dois dispositivos trocam a janela do receptor (ambos têm 32 KB neste caso). Eles também começam com 1 KB da janela de congestionamento, mas como o cliente será o único a enviar dados nesse exemplo, ele será o único usando significativamente esse valor. Na linha 2, o cliente recebe um ACK e dobra seu CWND (agora é 2k), e o servidor faz o mesmo ao receber um ACK na linha 3. Em seguida, o cliente envia dois segmentos de 1k de dados cada, que eles são reconhecidos posteriormente nas linhas 6 e 7, onde a janela de congestionamento no cliente é duplicada (4k, depois 8k). Em seguida, temos outra troca, onde o cliente envia 1k de dados que são imediatamente confirmados, efetivamente dobrando a janela de congestionamento novamente (agora 16k na linha 9). Isso é repetido na linha 10-11, onde o CWND atinge 32k. Neste ponto,a janela de congestionamento não pode crescer mais, a menos que a janela do receptor também aumente. Em vez disso, na linha 12, um segmento é perdido e, após um tempo limite (não mostrado na imagem), a janela de congestionamento e o ssthresh são definidos para 16k. Então, na linha 13-14, 1k de dados é enviado e confirmado, mas desta vez a janela de congestionamento não dobra, pois já é igual a ssthresh. Em vez disso, aumenta por seu valor inicial (1k). Na linha final 16, o servidor reduz sua janela de receptor para 8k, então a janela de congestionamento no cliente também é definida para 8k.já é igual a ssthresh. Em vez disso, aumenta por seu valor inicial (1k). Na linha final 16, o servidor reduz sua janela de receptor para 8k, então a janela de congestionamento no cliente também é definida para 8k.já é igual a ssthresh. Em vez disso, aumenta por seu valor inicial (1k). Na linha final 16, o servidor reduz sua janela de receptor para 8k, então a janela de congestionamento no cliente também é definida para 8k.

Este é um bom exemplo, mas vamos ver outro. Em vez de representar a troca do segmento, apresentaremos um gráfico mostrando a evolução da janela de congestionamento ao longo do tempo.

![Gráfico de janela de congestionamento TCP](https://www.ictshore.com/wp-content/uploads/2016/12/1017-09-TCP_congestion_window.png)Neste gráfico, temos a evolução da janela de congestionamento, seu limite (ssthresh) e a janela do receptor (rwnd).

Como você pode ver na imagem, a janela de congestionamento começa com o valor de 1 segmento e continua dobrando (recebendo ACKs) até atingir o ssthresh, então começa a aumentar linearmente até que um segmento seja perdido. Então, o ssthresh é definido para metade da janela de congestionamento máxima alcançada, e a janela de congestionamento continua a aumentar linearmente a partir dela. Finalmente, a janela do receptor é reduzida e a janela de ssthresh e congestionamento vai com ela.

### Predominância de UDP e privação de TCP

Com o controle de congestionamento em mente, estamos prontos para falar sobre **UDP e TCP na mesma rede** . O TCP implementa esse mecanismo sofisticado para recuar em caso de congestionamento da rede, mas o UDP não. Portanto, se muito tráfego for gerado em UDP, tanto que TCP e UDP juntos excedam a capacidade da rede, o *TCP recuará* devido ao algoritmo de controle de congestionamento. Em vez disso, o UDP continuará a usar a largura de banda que já estava usando e, se algum tráfego UDP foi enfileirado devido ao congestionamento da rede, ele será acionado agora que a rede não está mais congestionada. Se isso saturar a rede novamente, o TCP continuará a retroceder até que o UDP seja quase o único tráfego na rede. O gráfico a seguir explica isso.

![Predominância de UDP e privação de TCP](https://www.ictshore.com/wp-content/uploads/2016/12/1017-10-TCP_starvation-1.png)Cada vez que há congestionamento, o TCP recua usando a janela de congestionamento, deixando cada vez mais espaço para o tráfego UDP.

Este não é um problema real no ambiente de LAN, quando temos links de alta largura de banda que provavelmente não ficarão saturados. Em vez disso, em links WAN (conectando sites geograficamente distantes), devemos implementar alguns mecanismos para evitar que esses links fiquem saturados com tráfego UDP. Esses mecanismos são conhecidos como **Quality of Service (QoS)** , que é um conjunto de regras que definem como a rede deve reagir quando está congestionada. Essas regras são definidas em roteadores e basicamente definem os critérios pelos quais o tráfego pode ou não ser descartado. Lembre-se de que quando o link estiver cheio, algo deve ser descartado. As regras de QoS podem *manter o UDP separado do TCP* de modo que o UDP pode saturar apenas sua parte da rede, pode conceder uma porcentagem de largura de banda para alguns aplicativos ou pode até mesmo reservar largura de banda para outros aplicativos (o que significa que esta parte da largura de banda será usada apenas por alguns aplicativos, e caso não sejam usando-o, nada mais será capaz de usá-lo).

Neste artigo, cobrimos todos os recursos sofisticados do TCP. Com esse conhecimento, você agora está pronto para discutir como o TCP funciona em um nível quase CCNP. Além disso, esse conhecimento será muito útil para você ao falar sobre firewalls, e agora você está com a mentalidade certa para verificar o que o UDP pode fazer e sobre o que se tratam as camadas de sessão e apresentação, como faremos no próximo artigo.